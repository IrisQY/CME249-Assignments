{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Error-minimization and Projected Bellman Error-minimization \n",
    "###### Notation basics\n",
    "- State space $S = \\{s_1,s_2,...s_n\\}$, each of which can be fitted into feature functions $\\phi = [\\phi_1,...,\\phi_m]$\n",
    "- Action space with finite actions $A$\n",
    "- Fixed, stochastic policy $\\pi(a|s)$\n",
    "- Value function under $\\pi(a|s)$, $\\boldsymbol{v}_{\\pi}$\n",
    "- Approximation of value function as $\\boldsymbol{v}_{w} = w^T \\phi$\n",
    "- Expected reward $r(s,a)$ | $R_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) r(s,a)$\n",
    "- Transition probability $P(s,s',a)$ | $P_{\\pi}(s,s') = \\sum_{a \\in A} \\pi(a|s) P(s,s',a)$\n",
    "- Discount factor $\\gamma$\n",
    "- Bellman operator $\\boldsymbol{B}_{\\pi} v = R_{\\pi} + \\gamma P_{\\pi} v$\n",
    "- Projection operator $\\boldsymbol{\\Pi}_{\\phi}$\n",
    "###### Projection VF\n",
    "$$\\boldsymbol{\\Pi}_{\\phi} v_{\\pi}: \\boldsymbol{w}_{\\pi} = \\arg \\min_{\\boldsymbol{w}}d(v_{\\pi}, v_{\\boldsymbol{w}})$$\n",
    "###### Bellman Error-minimization VF\n",
    "$$\\boldsymbol{w}_{BE} = \\arg \\min_{\\boldsymbol{w}}d(\\boldsymbol{B}_{\\pi} v_{\\boldsymbol{w}}, v_{\\boldsymbol{w}})$$\n",
    "i.e.,\n",
    "$$\n",
    "\\begin{split}\n",
    "\\boldsymbol{w}_{BE} & = \\arg \\min_{\\boldsymbol{w}}d(v_{\\boldsymbol{w}}, R_{\\pi}+\\gamma P_{\\pi}v_{\\boldsymbol{w}})\\\\\n",
    "& = \\arg \\min_{\\boldsymbol{w}}d(\\Phi \\boldsymbol{w}, R_{\\pi}+\\gamma P_{\\pi}\\Phi \\boldsymbol{w})\\\\\n",
    "& = \\arg \\min_{\\boldsymbol{w}}d(\\Phi \\boldsymbol{w} - \\gamma P_{\\pi}\\Phi \\boldsymbol{w}, R_{\\pi})\\\\\n",
    "& = \\arg \\min_{\\boldsymbol{w}}d((\\Phi - \\gamma P_{\\pi}\\Phi) \\boldsymbol{w}, R_{\\pi})\n",
    "\\end{split}\n",
    "$$\n",
    "Let $A = \\Phi - \\gamma P_{\\pi}\\Phi$ and $R_{\\pi} =b$,\n",
    "$$\\boldsymbol{w}_{BE} = (A^T D A)^{-1} A^T D b$$\n",
    "###### Temporal Difference Error-minimization VF\n",
    "$$\\boldsymbol{w}_{TDE} = \\arg \\min_{\\boldsymbol{w}}\\mathbb{E}_{\\pi}[\\delta^2]$$\n",
    "###### Projected Bellman Error-minimization VF\n",
    "$$\\boldsymbol{w}_{PBE} = \\arg \\min_{\\boldsymbol{w}}d(\\Pi_{\\phi} \\boldsymbol{B}_{\\pi} v_{\\boldsymbol{w}}, v_{\\boldsymbol{w}})$$\n",
    "Given that $\\min_{\\boldsymbol{w}}d(\\Pi_{\\phi} \\boldsymbol{B}_{\\pi} v_{\\boldsymbol{w}}, v_{\\boldsymbol{w}}) = 0$, we can solve the linear equation directly:\n",
    "$$\\Pi_{\\phi} \\boldsymbol{B}_{\\pi} \\Phi \\boldsymbol{w}_{PBE} = \\Phi \\boldsymbol{w}_{PBE}$$\n",
    "$$\\Phi (\\Phi^T D \\Phi)^{-1} \\Phi^T D (R_{\\pi} + \\gamma P_{\\pi} \\Phi \\boldsymbol{w}_{PBE}) = \\Phi \\boldsymbol{w}_{PBE}$$\n",
    "$$ \\Phi^T D (R_{\\pi} + \\gamma P_{\\pi} \\Phi \\boldsymbol{w}_{PBE}) = (\\Phi^T D \\Phi)\\boldsymbol{w}_{PBE}$$\n",
    "$$ \\Phi^T D R_{\\pi} = \\Phi^T D (\\gamma P_{\\pi} \\Phi - \\Phi )\\boldsymbol{w}_{PBE}$$\n",
    "Let $A = \\Phi^T D (\\gamma P_{\\pi} \\Phi - \\Phi )$ and $b = \\Phi^T D R_{\\pi}$\n",
    "$$\\boldsymbol{w}_{PBE} = A^{-1}b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Gradient Theorem\n",
    "###### Notation Basics\n",
    "- Discount factor $\\gamma$\n",
    "- State $s_t$, action $a_t$ and reward $r_t$\n",
    "- Transition probability $P_{s,s'}^a$\n",
    "- Expected reward $R_s^a = \\mathbb{E}[r_t|s_t = s, a_t = a]$\n",
    "- Initial state distribution $p_0$\n",
    "- Policy function approx $\\pi(s,a|\\theta)$\n",
    "\n",
    "###### Obj. Function\n",
    "$$J(\\theta) = \\int_{s} p_0(s_0) V_{\\pi}(s_0) ds_0 = \\int_{s} p_0(s_0) \\int_a \\pi(s_0, a_0, \\theta) Q_{\\pi}(s_0, a_0) da_0 ds_0$$\n",
    "\n",
    "###### Proof of Policy Gradient Theorem\n",
    "$$\n",
    "\\begin{split}\n",
    "\\nabla_{\\theta} J(\\theta) & = \\nabla_{\\theta} \\int_{s} p_0(s_0) \\int_a \\pi(s_0, a_0, \\theta) Q_{\\pi}(s_0, a_0) da_0 ds_0\\\\\n",
    "& = \\int_{s} p_0(s_0) \\int_a \\nabla_{\\theta} \\pi(s_0, a_0, \\theta) Q_{\\pi}(s_0, a_0) da_0 ds_0  + \\int_{s} p_0(s_0) \\int_a \\pi(s_0, a_0, \\theta) \\nabla_{\\theta} Q_{\\pi}(s_0, a_0) da_0 ds_0 \\\\\n",
    "& = \\int_{s} p_0(s_0) \\int_a \\nabla_{\\theta} \\pi(s_0, a_0, \\theta) Q_{\\pi}(s_0, a_0) da_0 ds_0 + \\int_{s} p_0(s_0) \\int_a \\pi(s_0, a_0, \\theta) \\nabla_{\\theta} (R_{s_0}^{a_0} + \\gamma P_{s_0, s_1}^{a_0} V_{\\pi}(s_1)) da_0 ds_0 ds_1\\\\\n",
    "& = \\int_{s} p_0(s_0) \\int_a \\nabla_{\\theta} \\pi(s_0, a_0, \\theta) Q_{\\pi}(s_0, a_0) da_0 ds_0 + \\int_{s} p_0(s_0) \\int_a \\pi(s_0, a_0, \\theta) \\gamma \\int_{s} P_{s_0, s_1}^{a_0} \\nabla_{\\theta} V_{\\pi}(s_1) da_0 ds_0 ds_1\\\\\n",
    "& = \\int_{s} p_0(s_0) \\int_a \\nabla_{\\theta} \\pi(s_0, a_0, \\theta) Q_{\\pi}(s_0, a_0) da_0 ds_0 + \\int_{s} \\gamma \\int_{s} p_0(s_0) p(s_0,s_1; \\pi) ds_0 \\nabla_{\\theta} V_{\\pi}(s_1) ds_1\\\\\n",
    "& = \\int_{s} p_0(s_0) \\int_a \\nabla_{\\theta} \\pi(s_0, a_0, \\theta) Q_{\\pi}(s_0, a_0) da_0 ds_0 + \\int_{s} \\gamma \\int_{s} p_0(s_0) p(s_0,s_1; \\pi) ds_0 \\int_a \\pi(s_1,a_1|\\theta) \\nabla_{\\theta} Q_{\\pi}(s_1, a_1) da_1 ds_1\\\\\n",
    "& = ...\\\\\n",
    "& = \\sum_{t=0}^{\\infty} \\int_{s} \\int_{s} \\gamma^t p_0(s_0) p(s_0,s_t,t; \\pi) ds_0 \\int_a \\pi(s_t,a_t|\\theta) \\nabla_{\\theta} Q_{\\pi}(s_t, a_t) da_t ds_t\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score function for softmax policy\n",
    "$$\\pi(s,a|\\theta) = \\frac{\\exp(\\theta^T \\phi(s,a))}{\\sum_b \\exp(\\theta^T \\phi(s,b))}$$\n",
    "$$\\nabla_{\\theta} \\log \\pi(s,a|\\theta) = \\phi(s,a) - \\sum_b \\theta^T \\phi(s,b)$$\n",
    "#### Score function for gaussian policy\n",
    "$$\\pi(s,a|\\theta) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp(\\frac{a - \\theta^T \\phi(s)}{-2\\sigma^2})$$\n",
    "$$\\nabla_{\\theta} \\log \\pi(s,a|\\theta) =\\frac{(a - \\theta^T \\phi(s))\\phi(s)}{\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compatible Function Approximation Theorem\n",
    "If 1) critic gradient is compatible with the Actor score function:\n",
    "$$\\nabla_{\\theta} \\log \\pi(s,a|\\theta) = \\nabla_{w} Q(s,a,w)$$\n",
    "and 2) critic parameters w minimize the following mean-squared error:\n",
    "$$ \\int_s \\rho_{\\pi}(s) \\int_a \\pi(s,a,\\theta) (Q_{\\pi}(s,a) - Q(s,a,w))^2 da ds$$\n",
    "Then the Policy Gradient using critic $Q(s,a,w)$ is exact:\n",
    "$$\\nabla_{\\theta} J(\\theta) \\int_s \\rho_{\\pi}(s) \\int_a \\nabla_{\\theta} \\pi(s,a,\\theta) Q(s,a,w)$$\n",
    "\n",
    "###### Proof\n",
    "Following 2)\n",
    "$$ \\int_s \\rho_{\\pi}(s) \\int_a \\pi(s,a,\\theta) (Q_{\\pi}(s,a) - Q(s,a,w)) \\nabla_{w} Q(s,a,w) da ds = 0$$\n",
    "Following 1)\n",
    "$$ \\int_s \\rho_{\\pi}(s) \\int_a \\pi(s,a,\\theta) (Q_{\\pi}(s,a) - Q(s,a,w)) \\nabla_{\\theta} \\log \\pi(s,a|\\theta) da ds = 0$$\n",
    "i.e.,\n",
    "$$ \\int_s \\rho_{\\pi}(s) \\int_a \\pi(s,a,\\theta) Q_{\\pi}(s,a) \\nabla_{\\theta} \\log \\pi(s,a|\\theta) da ds = \\int_s \\rho_{\\pi}(s) \\int_a \\pi(s,a,\\theta) Q(s,a,w) \\nabla_{\\theta} \\log \\pi(s,a|\\theta) da ds$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REINFORCE Algoithm (Monte-Carlo Policy Gradient Algorithm, i.e., no Critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
