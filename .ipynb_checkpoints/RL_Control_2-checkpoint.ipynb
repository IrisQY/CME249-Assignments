{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from typing import TypeVar,Mapping, Set, Generic, Sequence, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RL interface with value function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPforRL_FA():\n",
    "    \n",
    "    # note that state and actions are defined as int in this part \n",
    "    def __init__(self, \n",
    "                 state_action_simulator: Callable[[int], int], \n",
    "                 state_reward_simulator: Callable[[(int,int)], int],\n",
    "                 init_state: Callable[[], int]\n",
    "                 gamma: float) -> None:\n",
    "        super(MDPforRL_FA, self).__init__()\n",
    "\n",
    "        self.init_state = init_state\n",
    "        self.state_action_func = state_action_simulator\n",
    "        self.state_reward_func = state_reward_simulator\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def gen_init_state(self): \n",
    "        return self.init_state()\n",
    "    \n",
    "    def gen_init_action(self):\n",
    "        return self.state_action_func()\n",
    "    \n",
    "    def gen_next_state_reward(self,S: int,A: int):\n",
    "        return self.state_reward_func(S,A)\n",
    "    \n",
    "class FA_RL_interface():\n",
    "\n",
    "    def __init__(self, mdp: MDPforRL_FA):\n",
    "        super(FA_RL_interface).__init__()\n",
    "        self.mdp = mdp\n",
    "    \n",
    "    # Generate initial step\n",
    "    def init_state_gen(self) -> int:\n",
    "        return mdp.gen_init_state()\n",
    "    def init_action_gen(self) -> int:\n",
    "        return mdp.gen_init_action()\n",
    "    \n",
    "    # Generate next step\n",
    "    def next_state_gen(self, cur_state: int, cur_act: int) -> tuple:\n",
    "        return mdp.gen_next_state_reward(cur_state,cur_act)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Prediction algorithm with Value Function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD Prediction algorithm with Value Function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD($\\lambda$) Prediction algorithm with Value Function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA with Value Function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning with Value Function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test implementions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
