{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from typing import TypeVar,Mapping, Set, Generic, Sequence, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface for tabular RL algorithms\n",
    "- The core of this interface should be a mapping from a (state, action) pair to a sampling of the (next state, reward) pair. It is important that this interface doesn't present the state-transition probability model or the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify MDP class\n",
    "class MDPforRL_TB():\n",
    "    \n",
    "    # note that state and actions are defined as int in this part \n",
    "    # extension 1: can create a dict of state2idx outside of the class\n",
    "    # extension 2: can change input simulator to s RV generation function (vs. pre-defined np)\n",
    "    def __init__(self, state_action_tab: dict, # get available actions from each state\n",
    "                 state_action_simulator: np.ndarray, # probability of [s_cur,a,s_next]\n",
    "                 reward_simulator: np.ndarray, # reward if [s_cur,a]\n",
    "                 terminal_states: list,\n",
    "                 gamma: float) -> None:\n",
    "        super(MDPforRL_TB, self).__init__()\n",
    "        self.state = state_action_tab.keys()\n",
    "        self.state_action_tab = state_action_tab\n",
    "        self.state_action_simulator = state_action_simulator\n",
    "        self.reward_simulator = reward_simulator\n",
    "        self.terminal_states = terminal_states\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_state_action_simulator():\n",
    "        return self.state_action_simulator\n",
    "    \n",
    "    def gen_init_state(self): # uniform start\n",
    "        init_state = np.random.choice(len(self.state_action_simulator), 1)[0]\n",
    "        return init_state\n",
    "    \n",
    "    def gen_next_state_reward(self,S,A):\n",
    "        u = np.random.uniform(0,1)\n",
    "        cdf = np.cumsum(self.state_action_simulator[S,A,:])\n",
    "        next_state = np.where(cdf > u)[0][0]\n",
    "        step_reward = self.reward_simulator[S,A]\n",
    "        return next_state, step_reward\n",
    "    \n",
    "    def get_avail_actions(self, S):\n",
    "        return self.state_action_tab[S]\n",
    "    \n",
    "    def get_state(self):\n",
    "        return list(self.state)\n",
    "    \n",
    "    def get_terminal_states(self):\n",
    "        return self.terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tab_RL_interface():\n",
    "\n",
    "    def __init__(self, mdp: MDPforRL_TB):\n",
    "        super(tab_RL_interface).__init__()\n",
    "        self.mdp = mdp\n",
    "#         self.num_episode= num_episode\n",
    "#         self.len_episode= len_episode\n",
    "#         self.init_method = init_method\n",
    "    \n",
    "    # Generate initial step\n",
    "    def init_state_gen(self) -> tuple:\n",
    "        return mdp.gen_init_state()\n",
    "    \n",
    "    # Generate next step\n",
    "    def next_state_gen(self, cur_state: int, cur_act: int) -> tuple:\n",
    "        return mdp.gen_next_state_reward(cur_state,cur_act)\n",
    "    \n",
    "    # Get available actions\n",
    "    def get_avail_actions(self, cur_state):\n",
    "        return mdp.get_avail_actions(cur_state)\n",
    "    \n",
    "    # Get states\n",
    "    def get_states(self):\n",
    "        return list(self.mdp.get_state())\n",
    "    def get_terminal_states(self):\n",
    "        return mdp.terminal_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every visit Monte-Carlo Value prediction\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\frac{1}{N(S_t)}(G_t - V(S_t))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate episodes by policy get_action() S->A\n",
    "def gen_episode(tb_rl:tab_RL_interface, num_episode: int, len_episode: int, \n",
    "                get_action: Callable[[int], int]) ->list:\n",
    "    # get_action: a rv generation function which takes a state index and generates an action index\n",
    "    MC_path = []\n",
    "    for i in range(num_episode):\n",
    "        trial = []\n",
    "        s_cur = tb_rl.init_state_gen()\n",
    "        act = get_action(s_cur)\n",
    "        s_next, reward = tb_rl.next_state_gen(s_cur,act)\n",
    "        trial.append((s_cur, act, reward))\n",
    "        for j in range(len_episode):\n",
    "            if s_next not in tb_rl.get_terminal_states():\n",
    "                s_cur = s_next\n",
    "                act = get_action(s_cur)\n",
    "                s_next, reward = tb_rl.next_state_gen(s_cur,act)\n",
    "                trial.append((s_cur, act, reward))\n",
    "        MC_path.append(trial)\n",
    "    \n",
    "    return MC_path # list of list of tuples (s_cur, action, reward)\n",
    "\n",
    "# Step 2: Get value function prediction\n",
    "def mc_prediction(tb_rl:tab_RL_interface, num_episode: int, len_episode: int, \n",
    "                  get_action: Callable[[int], int]) -> dict:\n",
    "    MC_path = gen_episode(tb_rl, num_episode, len_episode, get_action)\n",
    "    sum_return = dict()\n",
    "    count_return = dict()\n",
    "    for i in range(len(MC_path)):\n",
    "        for j,elem in enumerate(MC_path[i]):\n",
    "            if MC_path[i][j][0] in sum_return.keys():\n",
    "                sum_return[MC_path[i][j][0]] += MC_path[i][j][2]\n",
    "                count_return[MC_path[i][j][0]] += 1\n",
    "            else:\n",
    "                sum_return.update({MC_path[i][j][0]: MC_path[i][j][2]})\n",
    "                count_return.update({MC_path[i][j][0]: 1})\n",
    "    val = dict()\n",
    "    for s in sum_return.keys():\n",
    "        val[s] = sum_return[s]/count_return[s]\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-step TD algorithm Value Function prediction\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_prediction(tb_rl:tab_RL_interface, num_episode: int, len_episode: int, \n",
    "                  alpha: float, gamma: float, get_action: Callable[[int], int]) -> dict:\n",
    "    # initiate value function\n",
    "    val = {s: 0.0 for s in tb_rl.get_states()}\n",
    "    i = 0\n",
    "    while i < num_episode:\n",
    "        j = 0\n",
    "        s_cur = tb_rl.init_state_gen()\n",
    "        while j < len_episode:\n",
    "            act = get_action(s_cur)\n",
    "            if s_next not in tb_rl.get_terminal_states():\n",
    "                s_next, reward = tb_rl.next_state_gen(s_cur,act)\n",
    "                val[s_cur] += alpha*(reward + gamma*val[s_next] - val[s_cur])\n",
    "                s_cur = s_next\n",
    "                j += 1\n",
    "            else:\n",
    "                break\n",
    "        i += 1\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the above implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_tab = {0:[0,1,2],1:[0,1,2],\n",
    "                   2:[0,1,2],3:[0,1,2]} # get available actions from each state\n",
    "state_action_simulator = np.asarray([0.25]*48).reshape((4,3,4))# probability of [s_cur,a,s_next]\n",
    "reward_simulator = np.asarray([0,1,3,2,1,3,2,0,3,2,1,4]).reshape((4,3))# reward if [s_cur,a]\n",
    "gamma = 1\n",
    "terminal_states = []\n",
    "mdp = MDPforRL_TB(state_action_tab,state_action_simulator,reward_simulator,terminal_states,gamma)\n",
    "tb_rl = tab_RL_interface(mdp)\n",
    "def get_action(s):\n",
    "    if s == 0:\n",
    "        return 2\n",
    "    elif s == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2, 3), (1, 1, 1), (0, 2, 3), (1, 1, 1), (1, 1, 1), (3, 0, 2), (2, 0, 2), (0, 2, 3), (0, 2, 3), (0, 2, 3), (0, 2, 3)], [(2, 0, 2), (1, 1, 1), (3, 0, 2), (1, 1, 1), (1, 1, 1), (3, 0, 2), (2, 0, 2), (0, 2, 3), (3, 0, 2), (3, 0, 2), (2, 0, 2)], [(0, 2, 3), (1, 1, 1), (3, 0, 2), (1, 1, 1), (3, 0, 2), (3, 0, 2), (2, 0, 2), (0, 2, 3), (2, 0, 2), (2, 0, 2), (1, 1, 1)], [(0, 2, 3), (0, 2, 3), (2, 0, 2), (2, 0, 2), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (3, 0, 2), (3, 0, 2), (3, 0, 2)], [(2, 0, 2), (0, 2, 3), (0, 2, 3), (3, 0, 2), (3, 0, 2), (1, 1, 1), (1, 1, 1), (2, 0, 2), (2, 0, 2), (0, 2, 3), (0, 2, 3)], [(0, 2, 3), (0, 2, 3), (2, 0, 2), (0, 2, 3), (1, 1, 1), (2, 0, 2), (2, 0, 2), (0, 2, 3), (0, 2, 3), (1, 1, 1), (0, 2, 3)], [(3, 0, 2), (0, 2, 3), (3, 0, 2), (2, 0, 2), (2, 0, 2), (1, 1, 1), (2, 0, 2), (0, 2, 3), (1, 1, 1), (2, 0, 2), (1, 1, 1)], [(0, 2, 3), (1, 1, 1), (1, 1, 1), (0, 2, 3), (3, 0, 2), (0, 2, 3), (3, 0, 2), (3, 0, 2), (0, 2, 3), (2, 0, 2), (1, 1, 1)], [(2, 0, 2), (2, 0, 2), (1, 1, 1), (2, 0, 2), (2, 0, 2), (2, 0, 2), (2, 0, 2), (3, 0, 2), (1, 1, 1), (0, 2, 3), (3, 0, 2)], [(0, 2, 3), (0, 2, 3), (1, 1, 1), (0, 2, 3), (1, 1, 1), (2, 0, 2), (1, 1, 1), (0, 2, 3), (0, 2, 3), (2, 0, 2), (0, 2, 3)]]\n",
      "{0: 3.0, 2: 2.0, 3: 2.0, 1: 1.0}\n"
     ]
    }
   ],
   "source": [
    "MC_path = gen_episode(tb_rl, 10, 10, get_action)\n",
    "print(MC_path)\n",
    "print(mc_prediction(tb_rl, 10, 10, get_action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 6.4136523505484675, 1: 3.9193447595540682, 2: 5.190022426616782, 3: 5.009242945308076}\n"
     ]
    }
   ],
   "source": [
    "print(td_prediction(tb_rl, 10, 10, 0.1, 1, get_action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prove that fixed learning rate for MC is equivalent to an exponentially decaying average of episode returns\n",
    "Given that\n",
    "$$V^{k}(S_t) = V^{k-1}(S_t) + \\alpha (G_t^{k-1} -V^{k-1}(S_t))$$\n",
    "We may get\n",
    "$$\n",
    "\\begin{split}\n",
    "V^{k}(S_t) &= (1-\\alpha) V^{k-1}(S_t) + \\alpha G_t^{k-1}\\\\\n",
    "& = (1-\\alpha)(V^{k-2}(S_t)+\\alpha (G_t^{k-2} -V^{k-2}(S_t)))+ \\alpha G_t^{k-1}\\\\\n",
    "& = \\alpha G_t^{k-1} +(1-\\alpha)\\alpha G_t^{k-2} +...+ (1-\\alpha)^{k-1} \\alpha G_t^{0} +(1-\\alpha)^k V^{0}(S_t)\n",
    "\\end{split}\n",
    "$$\n",
    "Which is equivalent to an exponentially decaying average:  https://en.wikipedia.org/wiki/Exponential_smoothing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
