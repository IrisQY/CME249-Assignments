{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from typing import TypeVar,Mapping, Set, Generic, Sequence, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward-View TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import class\n",
    "class MDPforRL_TB():\n",
    "    \n",
    "    # note that state and actions are defined as int in this part \n",
    "    # extension 1: can create a dict of state2idx outside of the class\n",
    "    # extension 2: can change input simulator to s RV generation function (vs. pre-defined np)\n",
    "    def __init__(self, state_action_tab: dict, # get available actions from each state\n",
    "                 state_action_simulator: np.ndarray, # probability of [s_cur,a,s_next]\n",
    "                 reward_simulator: np.ndarray, # reward if [s_cur,a]\n",
    "                 gamma: float) -> None:\n",
    "        super(MDPforRL_TB, self).__init__()\n",
    "        self.state = state_action_tab.keys()\n",
    "        self.state_action_tab = state_action_tab\n",
    "        self.state_action_simulator = state_action_simulator\n",
    "        self.reward_simulator =  reward_simulator\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_state_action_simulator():\n",
    "        return self.state_action_simulator\n",
    "    \n",
    "    def gen_init_state(self): # uniform start\n",
    "        init_state = np.random.choice(len(self.state_action_simulator), 1)[0]\n",
    "        return init_state\n",
    "    \n",
    "    def gen_next_state_reward(self,S,A):\n",
    "        u = np.random.uniform(0,1)\n",
    "        cdf = np.cumsum(self.state_action_simulator[S,A,:])\n",
    "        next_state = np.where(cdf > u)[0][0]\n",
    "        step_reward = self.reward_simulator[S,A]\n",
    "        return next_state, step_reward\n",
    "    \n",
    "    def get_avail_actions(self, S):\n",
    "        return self.state_action_tab[S]\n",
    "    \n",
    "    def get_state(self):\n",
    "        return list(self.state)\n",
    "\n",
    "class tab_RL_interface():\n",
    "\n",
    "    def __init__(self, mdp: MDPforRL_TB):\n",
    "        super(tab_RL_interface).__init__()\n",
    "        self.mdp = mdp\n",
    "#         self.num_episode= num_episode\n",
    "#         self.len_episode= len_episode\n",
    "#         self.init_method = init_method\n",
    "    \n",
    "    # Generate initial step\n",
    "    def init_state_gen(self) -> tuple:\n",
    "        return mdp.gen_init_state()\n",
    "    \n",
    "    # Generate next step\n",
    "    def next_state_gen(self, cur_state: int, cur_act: int) -> tuple:\n",
    "        return mdp.gen_next_state_reward(cur_state,cur_act)\n",
    "    \n",
    "    # Get available actions\n",
    "    def get_avail_actions(self, cur_state):\n",
    "        return mdp.get_avail_actions(cur_state)\n",
    "    \n",
    "    # Get states\n",
    "    def get_states(self):\n",
    "        return list(self.mdp.get_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate episodes by policy get_action() S->A\n",
    "def gen_episode(tb_rl:tab_RL_interface, num_episode: int, len_episode: int, \n",
    "                get_action: Callable[[int], int]) ->list:\n",
    "    # get_action: a rv generation function which takes a state index and generates an action index\n",
    "    TD_path = []\n",
    "    for i in range(num_episode):\n",
    "        trial = []\n",
    "        s_cur = tb_rl.init_state_gen()\n",
    "        act = get_action(s_cur)\n",
    "        s_next, reward = tb_rl.next_state_gen(s_cur,act)\n",
    "        trial.append((s_cur, act, reward))\n",
    "        for j in range(len_episode):\n",
    "            s_cur = s_next\n",
    "            act = get_action(s_cur)\n",
    "            s_next, reward = tb_rl.next_state_gen(s_cur,act)\n",
    "            trial.append((s_cur, act, reward))\n",
    "        TD_path.append(trial)\n",
    "    \n",
    "    return TD_path # list of list of tuples (s_cur, action, reward)\n",
    "\n",
    "# Step 2: Get value function prediction\n",
    "def td_forward(tb_rl:tab_RL_interface, num_episode: int, len_episode: int,\n",
    "               alpha: float, gamma: float, _lambda: float, \n",
    "               get_action: Callable[[int], int]) -> dict:\n",
    "    \n",
    "    TD_path = gen_episode(tb_rl, num_episode, len_episode, get_action)\n",
    "    \n",
    "    # initiate value function\n",
    "    val = {s: 0.0 for s in tb_rl.get_states()}\n",
    "    for i in range(len(TD_path)):\n",
    "        G_path = np.zeros(len(TD_path[i]))\n",
    "        for j in range(len(TD_path[i])):\n",
    "            for k in range(j,len(TD_path[i])):\n",
    "                G_path[k] += gamma**j*TD_path[i][j][2]\n",
    "            G_path[j] += gamma**(j+1)*val(TD_path[i][j][0])\n",
    "        G_lambda = 0\n",
    "        for j in range(G_path.shape[0]):\n",
    "            G_lambda += (1-_lambda)*_lambda**j*G_path[j]\n",
    "        val[TD_path[i][0][0]] = val[TD_path[i][0][0]] + alpha*(G_lambda - val[TD_path[i][0][0]])\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward-View TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_backward(tb_rl:tab_RL_interface, num_episode: int, len_episode: int,\n",
    "               alpha: float, gamma: float, _lambda: float, \n",
    "               get_action: Callable[[int], int]) -> dict:\n",
    "        \n",
    "    # initiate value function\n",
    "    val = np.zeros(len(tb_rl.get_states()))\n",
    "    e_t = np.zeros(len(tb_rl.get_states()))\n",
    "    \n",
    "    i = 0\n",
    "    while i < num_episode:\n",
    "        j = 0\n",
    "        s_cur = tb_rl.init_state_gen()\n",
    "        while j < len_episode:\n",
    "            act = get_action(s_cur)\n",
    "            s_next, reward = tb_rl.next_state_gen(s_cur,act)\n",
    "            e_t *= _lambda * gamma\n",
    "            e_t[s_cur] += 1.0\n",
    "            td_error = reward + gamma * val[s_next] - val[s_cur]\n",
    "            val[s_cur] += alpha*(reward + gamma*val[s_next] - val[s_cur])\n",
    "            s_cur = s_next\n",
    "            val += alpha * td_error * e_t\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the above implementions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.53842121 10.08099553 10.1583265  10.54858894]\n"
     ]
    }
   ],
   "source": [
    "state_action_tab = {0:[0,1,2],1:[0,1,2],\n",
    "                   2:[0,1,2],3:[0,1,2]} # get available actions from each state\n",
    "state_action_simulator = np.asarray([0.25]*48).reshape((4,3,4))# probability of [s_cur,a,s_next]\n",
    "reward_simulator = np.asarray([0,1,3,2,1,3,2,0,3,2,1,4]).reshape((4,3))# reward if [s_cur,a]\n",
    "gamma = 1\n",
    "mdp = MDPforRL_TB(state_action_tab,state_action_simulator,reward_simulator,gamma)\n",
    "tb_rl = tab_RL_interface(mdp)\n",
    "def get_action(s):\n",
    "    if s == 0:\n",
    "        return 2\n",
    "    elif s == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "print(td_backward(tb_rl, 10, 10, 0.1, 1, 0.1, get_action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Forward-View and Backward-View TD\n",
    "We know offline forward-view TD($\\lambda$) as:\n",
    "$$G_t^{(n)} = R_{t+1}+...+\\gamma^n V(S_{t+n})$$\n",
    "$$G_t^{\\lambda} = (1-\\lambda)\\sum_{n=1}^{\\infty}\\lambda^{n-1} G_t^{(n)}$$\n",
    "$$V(S_t) = V(S_t) + \\alpha(G_t^{\\lambda} - V(S_t))$$\n",
    "And backward-view TD($\\lambda$) as:\n",
    "$$\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$$\n",
    "$$\\mathbb{E}_t(s) = \\gamma \\lambda \\mathbb{E}_{t-1}(s) + \\mathbb{1}(S_t = s)$$\n",
    "$$V(S_t) = V(S_t) + \\alpha \\delta_t \\mathbb{E}_t(s) | \\mathbb{E}_0(s) = 0$$\n",
    "Consider an episode where $s$ is visited once at time-step $k$,then $\\mathbb{E}_t(s) = 0$ for $t < k$ and $E_t(s) = (\\lambda \\gamma)^{t-k}$ for $t \\geq k$. We also know that for forward TD\n",
    "$$G_t^{\\lambda} - V(S_t) = \\delta_t + \\gamma \\lambda \\delta_{t+1} + (\\gamma \\lambda)^2 \\delta_{t+2} +...$$\n",
    "Thus for backward TD\n",
    "$$\\sum_{t=1}^T \\delta_t \\mathbb{E}_t(s) = G_t^{\\lambda} - V(S_t)$$\n",
    "i.e.,\n",
    "$V(S_t) + \\alpha(G_t^{\\lambda} - V(S_t))$ is equivalent to $V(S_t) + \\alpha \\delta_t \\mathbb{E}_t(s)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
