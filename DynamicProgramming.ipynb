{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from typing import TypeVar,Mapping, Set, Generic, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP algorithms\n",
    "- Write code for Policy Evaluation (tabular) algorithm\n",
    "- Write code for Policy Iteration (tabular) algorithm\n",
    "- Write code for Value Iteration (tabular) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **The code below is replicated in src folder with .py format** </font>\n",
    "#### Import old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "T = TypeVar(\"T\",str,int,float)\n",
    "\n",
    "# Identity helper function for str, int and float\n",
    "def ind(x: T, y: T):\n",
    "    if x == y or np.abs(x-y)<1e-5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Get state helper function\n",
    "def get_states_helper(in_graph: dict) -> dict:\n",
    "    state_list = list(in_graph.keys())\n",
    "    ind = range(len(state_list))\n",
    "    state = dict(zip(state_list,ind))\n",
    "    return state\n",
    "\n",
    "# Get transition matrix helper function\n",
    "def get_transition_helper(in_graph: dict) -> np.ndarray:\n",
    "    state = get_states_helper(in_graph)\n",
    "    tran_mat = np.zeros((len(state),len(state)))\n",
    "    for i, row in in_graph.items():\n",
    "        for j, prob in row.items():\n",
    "            ind_row = state[i]\n",
    "            ind_col = state[j]\n",
    "            if ind(tran_mat[ind_row,ind_col],0):\n",
    "                tran_mat[ind_row,ind_col] = prob\n",
    "    return tran_mat \n",
    "# Define MP by Graph\n",
    "\"\"\"\n",
    "    E.g.,\n",
    "    Input = {'Sunny': {'Sunny': 0.1, 'Cloudy': 0.2, 'Rainy': 0.3, 'Cloudy': 0.4},\n",
    "             'Cloudy': {'Sunny': 0.25, 'Cloudy': 0.25, 'Rainy': 0.3, 'Cloudy': 0.2},\n",
    "             'Rainy': {'Sunny': 0.1, 'Cloudy': 0.2, 'Rainy': 0.3, 'Cloudy': 0.4},\n",
    "             'Windy': {'Sunny': 0.25, 'Cloudy': 0.25, 'Rainy': 0.25, 'Cloudy': 0.25}}\n",
    "    Meaning: Today's weather => tmr's weather\n",
    "\"\"\"\n",
    "\n",
    "class MP:\n",
    "    # Initiate state dict & transition matrix\n",
    "    def __init__(self, in_graph: dict) -> None:\n",
    "        self.graph = in_graph\n",
    "        state = get_states_helper(in_graph)\n",
    "        tran_mat = get_transition_helper(in_graph)\n",
    "        # Check transition matrix and match state set with transition probs\n",
    "        if np.linalg.norm(np.sum(tran_mat, axis = 1)- np.ones(tran_mat.shape[0]))>1e-5:\n",
    "            raise ValueError\n",
    "        elif len(state) != tran_mat.shape[0]:\n",
    "            raise ValueError\n",
    "        else:\n",
    "            self.state: dict = state\n",
    "            self.tran_mat: np.ndarray = tran_mat\n",
    "            \n",
    "    # Get all states\n",
    "    def get_states(self) -> set:\n",
    "        return self.state\n",
    "    \n",
    "    # Get the transition matirx\n",
    "    def get_tran_mat(self) -> np.ndarray:\n",
    "        return self.tran_mat\n",
    "    \n",
    "    # Compute stationary distribution using eigenvalue decomposition\n",
    "    def stationary_dist(self) -> np.array:\n",
    "        e_value, e_vec = np.linalg.eig(self.tran_mat.T)\n",
    "        out = np.array(e_vec[:, np.where(np.abs(e_value- 1.) < 1e-5)[0][0]])\n",
    "        out = out/np.sum(out)\n",
    "        return out\n",
    "# Define MRP by Graph\n",
    "\"\"\"\n",
    "    E.g.,\n",
    "    Input = {'Sunny': {'Sunny': 0.1, 'Cloudy': 0.2, 'Rainy': 0.3, 'Cloudy': 0.4},\n",
    "             'Cloudy': {'Sunny': 0.25, 'Cloudy': 0.25, 'Rainy': 0.3, 'Cloudy': 0.2},\n",
    "             'Rainy': {'Sunny': 0.1, 'Cloudy': 0.2, 'Rainy': 0.3, 'Cloudy': 0.4},\n",
    "             'Windy': {'Sunny': 0.25, 'Cloudy': 0.25, 'Rainy': 0.25, 'Cloudy': 0.25}}\n",
    "    state_reward = {'Rain': 1, 'Sunny': 2, 'Cloudy': 3, 'Windy': 4}\n",
    "    gamma = 0.5\n",
    "    Meaning: Today's weather => tmr's weather\n",
    "\"\"\"\n",
    "class MRP(MP):\n",
    "    \n",
    "    # Initiate state with reward and discount\n",
    "    def __init__(self, in_graph: dict, state_reward: dict, gamma: float) -> None:\n",
    "        super().__init__(in_graph)\n",
    "        self.state = self.get_states()\n",
    "        self.tran_mat = self.get_tran_mat()\n",
    "        if gamma <0 or gamma >1:\n",
    "            raise ValueError\n",
    "        else:\n",
    "            reward_vec = np.zeros(len(self.state))\n",
    "            for key, ind in self.state.items():\n",
    "                reward_vec[ind] = state_reward[key]\n",
    "            \n",
    "            self.reward: np.ndarray = reward_vec\n",
    "            self.gamma: float = gamma\n",
    "    \n",
    "    # Get all states\n",
    "    def get_states(self) -> set:\n",
    "        return self.state\n",
    "    \n",
    "    # Get the transition matirx\n",
    "    def get_tran_mat(self) -> np.ndarray:\n",
    "        return self.tran_mat\n",
    "    \n",
    "    # Compute value function R(s)\n",
    "    def value_func(self) -> float:\n",
    "        return np.linalg.inv(np.identity(len(self.state))-self.gamma*self.tran_mat).dot(self.reward)\n",
    "\n",
    "    # Compute value function r(s,s')\n",
    "    def value_func_2nd(self,_2nd_def_reward) -> float:\n",
    "        reward_dict = convert_reward(_2nd_def_reward)\n",
    "        reward_vec = np.zeros(len(self.state))\n",
    "        for key, ind in self.state.items():\n",
    "            reward_vec[ind] = reward_dict[key]\n",
    "        self.reward = reward_vec\n",
    "        return self.value_func()\n",
    "\n",
    "# Get actions helper function\n",
    "def get_actions_helper(in_graph: dict) -> dict:\n",
    "    state = get_states_helper(in_graph)\n",
    "    actions_set = set()\n",
    "    for s in state:\n",
    "        temp_set = set(in_graph[s].keys())\n",
    "        actions_set.update(temp_set)\n",
    "    actions_list = list(actions_set)\n",
    "    ind = range(len(actions_list))\n",
    "    actions = dict(zip(actions_list,ind))\n",
    "    return actions\n",
    "\n",
    "# Get transition matrix helper function\n",
    "def get_transition_helper_mdp(in_graph: dict) -> np.ndarray:\n",
    "    states = get_states_helper(in_graph)\n",
    "    actions = get_actions_helper(in_graph)\n",
    "    tran_mat = np.zeros((len(states),len(actions),len(states))) # States * actions * states\n",
    "    for i, row in in_graph.items():\n",
    "        for c, action in row.items():\n",
    "            for j, prob in action.items():\n",
    "                ind_row = states[i]\n",
    "                ind_height = states[j]\n",
    "                ind_col = actions[c]\n",
    "                if ind(tran_mat[ind_row,ind_col,ind_height],0):\n",
    "                    tran_mat[ind_row,ind_col,ind_height] = prob\n",
    "    return tran_mat \n",
    "\n",
    "# Get reward matrix helper function\n",
    "def get_reward_helper(in_graph: dict, state_action_reward: dict) -> np.ndarray:\n",
    "    states = get_states_helper(in_graph)\n",
    "    actions = get_actions_helper(in_graph)\n",
    "    reward_mat = np.zeros((len(states),len(actions))) # States * actions\n",
    "    for i, row in state_action_reward.items():\n",
    "        for j, reward in row.items():\n",
    "            ind_row = states[i]\n",
    "            ind_col = actions[j]\n",
    "            if ind(reward_mat[ind_row,ind_col],0):\n",
    "                reward_mat[ind_row,ind_col] = reward\n",
    "    return reward_mat\n",
    "\n",
    "# Get policy matrix helper function\n",
    "def get_policy_helper(in_graph: dict, policy: dict) -> np.ndarray:\n",
    "    states = get_states_helper(in_graph)\n",
    "    actions = get_actions_helper(in_graph)\n",
    "    policy_mat = np.zeros((len(states),len(actions)))\n",
    "    for i, row in policy.items():\n",
    "        for j, prob in row.items():\n",
    "            ind_row = states[i]\n",
    "            ind_col = actions[j]\n",
    "            if ind(policy_mat[ind_row,ind_col],0):\n",
    "                policy_mat[ind_row,ind_col] = prob\n",
    "    return policy_mat\n",
    "\n",
    "class MDP():\n",
    "\n",
    "    def __init__(self, in_graph: dict, state_action_reward: dict, policy: dict, gamma: float) -> None:\n",
    "        self.state: dict = get_states_helper(in_graph)\n",
    "        self.action: dict = get_actions_helper(in_graph)\n",
    "        self.tran_mat: np.ndarray = get_transition_helper_mdp(in_graph)\n",
    "        self.reward: np.ndarray = get_reward_helper(in_graph,state_action_reward)\n",
    "        self.policy: np.ndarray = get_policy_helper(in_graph,policy)\n",
    "        self.gamma: float = gamma\n",
    "\n",
    "    # Get all states\n",
    "    def get_states(self) -> dict:\n",
    "        return self.state\n",
    "\n",
    "    # Get the transition matrix\n",
    "    def get_tran_mat(self) -> np.ndarray:\n",
    "        return self.tran_mat\n",
    "\n",
    "    # Get all actions\n",
    "    def get_actions(self) -> dict:\n",
    "        return self.action\n",
    "\n",
    "    # Get reward mat\n",
    "    def get_reward(self) -> np.ndarray:\n",
    "        return self.reward\n",
    "\n",
    "    # Get policy mat\n",
    "    def get_policy(self) -> np.ndarray:\n",
    "        return self.policy\n",
    "    \n",
    "    # Get gamma\n",
    "    def get_gamma(self) -> float:\n",
    "        return self.gamma\n",
    "    \n",
    "    # Generate MRP\n",
    "    def generate_MRP(self, policy: dict):\n",
    "        print(self.reward)\n",
    "        print(self.policy)\n",
    "        R_S = dict(zip(list(self.state.keys()),[0]*len(self.state)))\n",
    "        P_S = np.zeros((len(self.state),len(self.state)))\n",
    "        in_graph = dict()\n",
    "        for s_cur, i in self.state.items():\n",
    "            tmp = 0\n",
    "            value = dict()\n",
    "            in_graph.update({s_cur:value})\n",
    "            for a, j in self.action.items():\n",
    "                R_S[s_cur] += self.reward[i,j]*self.policy[i,j]\n",
    "                for s_next, c in self.state.items():\n",
    "                    if s_next in value:\n",
    "                        value[s_next] += self.tran_mat[i,j,c]*self.policy[i,j]\n",
    "                    else:\n",
    "                        value.update({s_next: self.tran_mat[i,j,c]*self.policy[i,j]})\n",
    "        return MRP(in_graph,R_S,self.gamma)\n",
    "\n",
    "    # Compute state value function v_{\\pi}(s)\n",
    "    def state_value_func(self, policy) -> float:\n",
    "        MRP_tmp = self.generate_MRP(policy)\n",
    "        return np.linalg.inv(np.identity(len(MRP_tmp.state))-MRP_tmp.gamma*MRP_tmp.tran_mat).dot(MRP_tmp.reward)\n",
    "\n",
    "    # Compute action value function q_{\\pi}(s,a)\n",
    "    def action_value_func(self, policy) -> float:\n",
    "        out = np.zeros((len(self.state),len(self.action)))\n",
    "        for s_cur,i in self.state.items():\n",
    "            for a,j in self.action.items():\n",
    "                out[i,j] += self.reward[i,j]\n",
    "                for s_next,c in self.state.items():\n",
    "                    out[i,j] += self.gamma*self.tran_mat[i,j,c]*self.state_value_func(policy)[c]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New DP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function\n",
    "def policy_mat_transform(mdp: MDP, policy:dict)-> np.ndarray:\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.get_actions()\n",
    "    policy_mat = np.zeros((len(states),len(actions)))\n",
    "    for i, row in policy.items():\n",
    "        for j, prob in row.items():\n",
    "            ind_row = states[i]\n",
    "            ind_col = actions[j]\n",
    "            if ind(policy_mat[ind_row,ind_col],0):\n",
    "                policy_mat[ind_row,ind_col] = prob\n",
    "    return policy_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Policy evaluation\n",
    "# if input policy is in dict format\n",
    "# policy_mat = policy_mat_transform(mdp, policy) \n",
    "def policy_eval(mdp: MDP, policy_mat: np.ndarray, eps = 1e-10) -> np.ndarray:\n",
    "    reward = mdp.get_reward()\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.get_actions()\n",
    "    tran_mat = mdp.get_tran_mat()\n",
    "    gamma = mdp.get_gamma()\n",
    "    reward_pi = np.zeros(len(states))\n",
    "    tran_mat_pi = np.zeros((len(states),len(states)))\n",
    "    for a,i in actions.items():\n",
    "        reward_pi += policy_mat[:,i]*reward[:,i]\n",
    "        tran_mat_pi += policy_mat[:,i]*tran_mat[:,i,:]\n",
    "    reward_pi.reshape(-1,1)\n",
    "    tran_mat_pi.reshape(-1,1)\n",
    "    val = np.zeros_like(reward_pi)\n",
    "    i = 0\n",
    "    while True: \n",
    "        i += 1\n",
    "        memo = val.copy()\n",
    "        val = reward_pi + gamma * tran_mat_pi.dot(val)\n",
    "        if np.linalg.norm(val - memo) < eps:\n",
    "            break\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Policy iteration\n",
    "def policy_iter(mdp: MDP, policy_eval_func = policy_eval, epa = 1e-10) -> np.ndarray:\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.get_actions()\n",
    "    tran_mat = mdp.get_tran_mat()\n",
    "    policy = [1/len(actions)]*(len(states)*len(actions)).reshape((len(states),len(actions)))\n",
    "    while True:\n",
    "        val = policy_eval_fn(mdp, policy)\n",
    "        q_func = mdp.action_value_func(policy)\n",
    "        for s,i in states.items():\n",
    "            a_ind = np.argmax(policy[i,:])\n",
    "            for a, j in actions.items():\n",
    "                if j == a_ind:\n",
    "                    policy[i,j] = 1\n",
    "                else:\n",
    "                    policy[i,j] = 0\n",
    "        new_val = policy_eval_fn(mdp, policy)\n",
    "        if np.linalg.norm(val-new_val)<eps:\n",
    "            break\n",
    "        val = new_val\n",
    "    return policy, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Value iteration\n",
    "def value_iter(mdp: MDP, eps = 1e-8):\n",
    "    states = mdp.get_states()\n",
    "    actions = mdp.get_actions()\n",
    "    tran_mat = mdp.get_tran_mat()\n",
    "    cur_val = np.zeros(len(states))\n",
    "    det = 0\n",
    "    while det == 0 or np.abs(det)>eps:\n",
    "        for s in range(len(states)):\n",
    "            action_vals = np.zeros(len(actions))\n",
    "            for a, i in actions.items():\n",
    "                for s_next, j in states.items():\n",
    "                    action_vals[i] = tran_mat[states[s],i,j]*(reward[states[s],i] + mdp.gamma*val[j])\n",
    "            action_val_max = np.max(action_vals)\n",
    "            delta = max(delta, np.abs(action_val_max - cur_val[s]))\n",
    "            cur_val[s] = action_val_max       \n",
    "    \n",
    "    policy = np.zeros((len(states),len(actions)))\n",
    "    for s in range(len(states)):\n",
    "        action_vals = np.zeros(len(actions))\n",
    "        for a, i in actions.items():\n",
    "            for s_next, j in states.items():\n",
    "                action_vals[i] = tran_mat[states[s],i,j]*(reward[states[s],i] + mdp.gamma*val[j])\n",
    "        a_ast = np.argmax(action_vals)\n",
    "        policy[s, action_vals] = 1\n",
    "    \n",
    "    return policy, cur_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_rule(state_cub,move,s):\n",
    "    ind = np.where(state_cub == s)\n",
    "    ind_1 = ind[0] + move[0]\n",
    "    ind_2 = ind[1] + move[1]\n",
    "    if ind_1 <0 or ind_1 >= 4 or ind_2 <0 or ind_2 >= 4:\n",
    "        ind_1 = ind[0]\n",
    "        ind_2 = ind[1]        \n",
    "    return state_cub[ind_1,ind_2]\n",
    "state = dict(zip(range(16),range(16)))\n",
    "state_cub = np.asarray(range(16)).reshape((4,4))\n",
    "action = dict(zip([(1,0),(-1,0),(0,-1),(0,1)],range(4)))\n",
    "state_reward = dict(zip(range(16),list([0]+[-1]*14+[0])))\n",
    "Input = {}\n",
    "for s_cur, i in state.items():\n",
    "    act = {}\n",
    "    Input.update({s_cur:act})\n",
    "    for a, j in action.items():\n",
    "        _next = {}\n",
    "        act.update({a: _next})\n",
    "        for s_next, c in state.items():\n",
    "            if s_next == transition_rule(state_cub,a,s_cur):\n",
    "                _next.update({s_next:1})\n",
    "            else:\n",
    "                _next.update({s_next:0})\n",
    "state_action_reward = {}\n",
    "for s_cur, i in state.items():\n",
    "    act = {}\n",
    "    state_action_reward.update({s_cur:act})\n",
    "    for a, j in action.items():\n",
    "        s_next = transition_rule(state_cub,a,s_cur)\n",
    "        reward = state_reward[s_next[0]]\n",
    "        act.update({a:reward})\n",
    "policy = {}\n",
    "for s_cur, i in state.items():\n",
    "    act = {}\n",
    "    policy.update({s_cur:act})\n",
    "    for a, j in action.items():\n",
    "        act.update({a:0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(Input,state_action_reward,policy,gamma = 1)\n",
    "policy_mat = policy_mat_transform(mdp, policy)\n",
    "policy_mat[0,0:4] = [0,0,0,0]\n",
    "policy_mat[15,0:4] = [0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -6.5 -13.  -19.  -21.  -13.  -17.  -19.  -19.  -19.  -19.  -17.  -13.\n",
      " -21.  -19.  -13.   -6.5]\n"
     ]
    }
   ],
   "source": [
    "print(policy_eval(mdp, policy_mat, eps = 1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
