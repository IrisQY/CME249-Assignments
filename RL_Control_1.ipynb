{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from typing import TypeVar,Mapping, Set, Generic, Sequence, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy Policy Improvement Theorem\n",
    "Definition of $\\epsilon$-greedy: $\\pi(a|s) = \\epsilon/m + 1 -\\epsilon$ for $a = \\arg \\max_a Q(s,a)$ and $\\pi(a|s) = \\epsilon/m$ otherwise.\n",
    "Proof: If $\\pi'$ is the $\\epsilon$-greedy improvement from $Q_{\\pi}(s,a)$.\n",
    "$$\n",
    "\\begin{split}\n",
    "Q_{\\pi}(s,\\pi'(s)) &= \\sum_{a \\in A} \\pi'(a|s) Q_{\\pi}(s,a)\\\\\n",
    "&= \\epsilon/m \\sum_{a \\in A} Q_{\\pi}(s,a) + (1 -\\epsilon) \\max_{a \\in A} Q_{\\pi}(s,a)\\\\\n",
    "& \\geq \\epsilon/m \\sum_{a \\in A} Q_{\\pi}(s,a) + (1 -\\epsilon) \\sum_{a \\in A} \\frac{\\pi(a|s) - \\epsilon/m}{1 -\\epsilon}Q_{\\pi}(s,a)\\\\\n",
    "& \\geq \\sum_{a \\in A}\\pi(a|s) Q_{\\pi}(s,a)\\\\\n",
    "&= V_{\\pi}(s)\n",
    "\\end{split}\n",
    "$$\n",
    "Thus $V_{\\pi'}(s) \\geq V_{\\pi}(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy in the Limit with Infinite Exploration\n",
    "- All state-action pairs are explored infinitely many times (ensure exploration)\n",
    "$$\\lim_{k \\rightarrow \\infty} N_k (s,a) = \\infty$$\n",
    "- The policy converges on a greedy policy (ensure explotation)\n",
    "$$\\lim_{k \\rightarrow \\infty} \\pi_k (a|s) = \\mathbb{1}(a = \\arg \\max_{a' \\in A} Q_k (s,a'))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sarsa\n",
    "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha (R + \\gamma Q(S',A') -Q(S,A))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On-policy Sarsa\n",
    "# Step 1: generate epsilon greedy policy\n",
    "def epsilon_greedy(Q: np.ndarray, S:int, eps:float) -> np.ndarray:\n",
    "    pol = eps*np.ones(Q.shape[1])/Q.shape[1]\n",
    "    A_ast = np.argmax(Q[S])\n",
    "    pol[A_ast] += 1-eps\n",
    "    return pol\n",
    "\n",
    "# Step 2: prepare interface\n",
    "class MDPforRL_TB():\n",
    "    \n",
    "    # note that state and actions are defined as int in this part \n",
    "    # extension 1: can create a dict of state2idx outside of the class\n",
    "    # extension 2: can change input simulator to s RV generation function (vs. pre-defined np)\n",
    "    def __init__(self, state_action_tab: dict, # get available actions from each state\n",
    "                 state_action_simulator: np.ndarray, # probability of [s_cur,a,s_next]\n",
    "                 reward_simulator: np.ndarray, # reward if [s_cur,a]\n",
    "                 gamma: float) -> None:\n",
    "        super(MDPforRL_TB, self).__init__()\n",
    "        self.state = state_action_tab.keys()\n",
    "        self.action = state_action_tab[0]\n",
    "        self.state_action_tab = state_action_tab\n",
    "        self.state_action_simulator = state_action_simulator\n",
    "        self. reward_simulator =  reward_simulator\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_state_action_simulator():\n",
    "        return self.state_action_simulator\n",
    "    \n",
    "    def gen_init_state(self): # uniform start\n",
    "        init_state = np.random.choice(len(self.state_action_simulator), 1)[0]\n",
    "        return init_state\n",
    "    \n",
    "    def gen_next_state_reward(self,S,A):\n",
    "        u = np.random.uniform(0,1)\n",
    "        cdf = np.cumsum(self.state_action_simulator[S,A,:])\n",
    "        next_state = np.where(cdf > u)[0][0]\n",
    "        step_reward = self.reward_simulator[S,A]\n",
    "        return next_state, step_reward\n",
    "    \n",
    "    def get_avail_actions(self, S):\n",
    "        return self.state_action_tab[S]\n",
    "    \n",
    "    def get_state(self):\n",
    "        return list(self.state)\n",
    "    \n",
    "    def n_S(self):\n",
    "        return len(list(self.state))\n",
    "    \n",
    "    def n_A(self):\n",
    "        return len(list(self.action))\n",
    "    \n",
    "class tab_RL_interface():\n",
    "\n",
    "    def __init__(self, mdp: MDPforRL_TB):\n",
    "        super(tab_RL_interface).__init__()\n",
    "        self.mdp = mdp\n",
    "    \n",
    "    # Generate initial step\n",
    "    def init_state_gen(self) -> tuple:\n",
    "        return mdp.gen_init_state()\n",
    "    \n",
    "    # Generate next step\n",
    "    def next_state_gen(self, cur_state: int, cur_act: int) -> tuple:\n",
    "        return mdp.gen_next_state_reward(cur_state,cur_act)\n",
    "    \n",
    "    # Get available actions\n",
    "    def get_avail_actions(self, cur_state):\n",
    "        return mdp.get_avail_actions(cur_state)\n",
    "    \n",
    "    # Get states\n",
    "    def get_states(self):\n",
    "        return list(self.mdp.get_state())\n",
    "    \n",
    "    def n_S(self):\n",
    "        return self.mdp.n_S()\n",
    "    \n",
    "    def n_A(self):\n",
    "        return self.mdp.n_A()\n",
    "\n",
    "# Step 3: Sarsa\n",
    "def sarsa(tb_rl:tab_RL_interface, num_episode: int, len_episode: int, \n",
    "          alpha: float, gamma: float, epsilon_greedy: Callable[[int], int], eps: float) -> dict:\n",
    "    \n",
    "    # Initiate q_func table\n",
    "    q_func = np.zeros((tb_rl.n_S(), tb_rl.n_A()))\n",
    "    i = 0\n",
    "    while i < num_episode:\n",
    "        j = 0\n",
    "        s_cur = tb_rl.init_state_gen()\n",
    "        act_cur = epsilon_greedy(q_func, s_cur, eps)\n",
    "        while j < len_episode:\n",
    "            s_next, reward = tb_rl.next_state_gen(s_cur,act)\n",
    "            act_next = epsilon_greedy(q_func, s_next, eps)\n",
    "            q_func[s_cur, act_cur] += alpha*(reward + gamma*q_func[s_next, act_next] - q_func[s_cur, act_cur])\n",
    "            s_cur = s_next\n",
    "            act_cur = act_next\n",
    "            j += 1\n",
    "        i += 1\n",
    "    \n",
    "    return q_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward view Sarsa($\\lambda$):\n",
    "$$Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha (q_t^{\\lambda} -Q(S_t,A_t))$$\n",
    "$$q_t^{\\lambda} = (1-\\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} q_t^{(n)}$$\n",
    "$$q_t^{(n)} = R_{t+1} + \\gamma R_{t+2}+...+\\gamma^n Q(S_{t+n})$$\n",
    "- Backward view Sarsa($\\lambda$):\n",
    "$$\\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$$\n",
    "$$\\mathbb{E}_t(s,a) = \\gamma \\lambda \\mathbb{E}_{t-1}(s,a) + \\mathbb{1}(S_t = s, A_t = a)$$\n",
    "$$Q(s, a) = Q(s,a) + \\alpha \\delta_t \\mathbb{E}_t(s, a) | \\mathbb{E}_0(s, a) = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 & 2: generate epsilon greedy policy and prepare interface as above\n",
    "# Step 3: Backward Sarsa\n",
    "def Sarsa_backward(tb_rl:tab_RL_interface, num_episode: int, len_episode: int,\n",
    "               alpha: float, gamma: float, _lambda: float, eps: float,\n",
    "               epsilon_greedy: Callable[[int], int]) -> dict:\n",
    "\n",
    "    # Initiate q_func table\n",
    "    q_func = np.zeros((tb_rl.n_S(), tb_rl.n_A()))\n",
    "    e_t = np.zeros((tb_rl.n_S(), tb_rl.n_A()))\n",
    "    \n",
    "    i = 0\n",
    "    while i < num_episode:\n",
    "        j = 0\n",
    "        s_cur = tb_rl.init_state_gen()\n",
    "        a_cur = epsilon_greedy(q_func, s_cur, eps)\n",
    "        while j < len_episode:\n",
    "            s_next, reward = tb_rl.next_state_gen(s_cur,a_cur)\n",
    "            a_next = epsilon_greedy(q_func, s_next, eps)\n",
    "            e_t *= _lambda * gamma\n",
    "            e_t[s_cur, a_cur] += 1.0\n",
    "            error = reward + gamma * q_func[s_next, a_next] - q_func[s_cur, a_cur]\n",
    "            q_func[s_cur, a_cur] += alpha*(reward + gamma*q_func[s_next, a_next] - q_func[s_cur, a_cur])\n",
    "            s_cur = s_next\n",
    "            a_cur = a_next\n",
    "            q_func += alpha * error * e_t\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return q_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha (R + \\gamma \\max_{A'} Q(S',A') -Q(S,A))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 & 2: generate epsilon greedy policy and prepare interface as above\n",
    "# Step 3: Q-learning\n",
    "def greedy(Q: np.ndarray, S:int) -> int:\n",
    "    A_ast = np.argmax(Q[S])\n",
    "    return A_ast\n",
    "\n",
    "def q_learning(tb_rl:tab_RL_interface, num_episode: int, len_episode: int, \n",
    "               alpha: float, gamma: float, epsilon_greedy: Callable[[int], int], greedy: Callable[[int], int], \n",
    "               eps: float) -> dict:\n",
    "    \n",
    "    # Initiate q_func table\n",
    "    q_func = np.zeros((tb_rl.n_S(), tb_rl.n_A()))\n",
    "    i = 0\n",
    "    while i < num_episode:\n",
    "        j = 0\n",
    "        s_cur = tb_rl.init_state_gen()\n",
    "        while j < len_episode:\n",
    "            act_cur = epsilon_greedy(q_func, s_cur, eps)\n",
    "            s_next, reward = tb_rl.next_state_gen(s_cur,act)\n",
    "            act_next = greedy(q_func, s_next)\n",
    "            q_func[s_cur, act_cur] += alpha*(reward + gamma*q_func[s_next, act_next] - q_func[s_cur, act_cur])\n",
    "            s_cur = s_next\n",
    "            j += 1\n",
    "        i += 1\n",
    "    \n",
    "    return q_func"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
