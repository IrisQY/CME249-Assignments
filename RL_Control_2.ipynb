{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from typing import TypeVar,Mapping, Set, Generic, Sequence, Callable, Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RL interface with value function approximation\n",
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPforRL_FA():\n",
    "    \n",
    "    # note that state and actions are defined as int in this part \n",
    "    def __init__(self, \n",
    "                 state_action_simulator: Callable[[int], int], \n",
    "                 v_func_simulator: Callable[[int], int], \n",
    "                 q_func_simulator: Callable[[Tuple[int, int]], int],\n",
    "                 init_state: Callable[[], int],\n",
    "                 state_reward_func: Callable[[Tuple[int, int]], Tuple[int, float]],\n",
    "                 gamma: float) -> None:\n",
    "        super(MDPforRL_FA, self).__init__()\n",
    "\n",
    "        self.init_state = init_state\n",
    "        self.state_action_func = state_action_simulator\n",
    "        self.state_v_func = v_func_simulator\n",
    "        self.state_q_func = q_func_simulator\n",
    "        self.state_reward_func = state_reward_func\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def gen_init_state(self): \n",
    "        return self.init_state()\n",
    "    \n",
    "    def gen_action(self, S: int):\n",
    "        return self.state_action_func(S)\n",
    "    \n",
    "    def gen_v_func(self,S: int):\n",
    "        return self.state_v_func(S) # for example, linear approximation\n",
    "    \n",
    "    def gen_q_func(self,S: int,A: int):\n",
    "        return self.state_q_func(S,A) # for example, linear approximation\n",
    "    \n",
    "    def gen_state_reward(self, S: int, A:int):\n",
    "        return self.state_reward_func(S,A)\n",
    "    \n",
    "class FA_RL_interface():\n",
    "\n",
    "    def __init__(self, mdp: MDPforRL_FA, features: Callable[[int], List[float]]):\n",
    "        super(FA_RL_interface).__init__()\n",
    "        self.mdp = mdp\n",
    "        self.feature_func = features\n",
    "    \n",
    "    # Generate initial step\n",
    "    def init_state_gen(self) -> int:\n",
    "        return mdp.gen_init_state()\n",
    "    def action_gen(self,S: int) -> int:\n",
    "        return mdp.gen_action(S)\n",
    "    \n",
    "    def gen_action(self, S: int):\n",
    "        return mdp.gen_action(S)\n",
    "    \n",
    "    # Generate next step \n",
    "    def gen_state_reward(self, S: int, A:int):\n",
    "        return mdp.state_reward_func(S,A)\n",
    "    \n",
    "    # Etimate value\n",
    "    def gen_v_func(self,S: int):\n",
    "        return mdp.state_v_func(S)\n",
    "\n",
    "    def gen_q_func(self,S: int,A: int):\n",
    "        return mdp.state_q_func(S,A)\n",
    "    \n",
    "    # Generate features\n",
    "    def gen_features(self, S:int):\n",
    "        return self.feature_func(S)\n",
    "\n",
    "class linearFA():\n",
    "    def __init__(self, lr: float, features: np.ndarray):\n",
    "        self.lr = lr\n",
    "        self.features = features\n",
    "        self.n_features = self.features.shape[0]\n",
    "        self.params = np.zeros(self.n_features)\n",
    "    \n",
    "    def v_func_predict(self, new_feature):\n",
    "        return self.params.dot(new_feature)\n",
    "    \n",
    "    def update_params(self, new_feature, vf):\n",
    "        return (self.v_func_predict(new_feature) - vf)*new_feature # gradient\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte-Carlo Prediction algorithm with Value Function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate episodes by policy get_action() S->A\n",
    "def gen_episode(fa_rl:FA_RL_interface, num_episode: int, len_episode: int, \n",
    "                get_action: Callable[[int], int]) ->list:\n",
    "    # get_action: policy, a rv generation function which takes a state index and generates an action index\n",
    "    MC_path = []\n",
    "    for i in range(num_episode):\n",
    "        trial = []\n",
    "        s_cur = fa_rl.init_state_gen()\n",
    "        act = get_action(s_cur) # for example, eps-greedy\n",
    "        s_next, reward = fa_rl.gen_state_reward(s_cur,act)\n",
    "        trial.append((s_cur, act, reward))\n",
    "        for j in range(len_episode):\n",
    "            s_cur = s_next\n",
    "            act = get_action(s_cur)\n",
    "            s_next, reward = tb_rl.next_state_gen(s_cur,act)\n",
    "            trial.append((s_cur, act, reward))\n",
    "        MC_path.append(trial)\n",
    "    \n",
    "    return MC_path # list of list of tuples (s_cur, action, reward)\n",
    "\n",
    "# Step 2: Get value function prediction\n",
    "def mc_prediction(fa_rl:FA_RL_interface, num_episode: int, len_episode: int, \n",
    "                  get_action: Callable[[int], int], linear_FA: linearFA):\n",
    "    MC_path = gen_episode(fa_rl, num_episode, len_episode, get_action)\n",
    "    Rewards = np.zeros((len(MC_path),len(MC_path[1])))\n",
    "    States = np.zeros((len(MC_path),len(MC_path[1])))\n",
    "    for n in range((MC_path)):\n",
    "        for i in range(len(MC_path[1])):\n",
    "            Rewards[n,i] = MC_path[n][i][2]\n",
    "            States[n,i] = MC_path[n][i][2]\n",
    "    Return = np.zeros((len(MC_path),len(MC_path[1])))\n",
    "    for n in range((MC_path)):\n",
    "        for i in range(len(MC_path[1])):\n",
    "            for j in range(i,len(MC_path[1])):\n",
    "                Return[n,i] += Reward[j]*fa_rl.gamma**(j-i)\n",
    "        for i in range(len(MC_path[1])):\n",
    "            linear_FA.update_params(fa_rl.gen_features(States[n,i]),Return[n,i])\n",
    "    return linear_FA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD Prediction algorithm with Value Function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_prediction(fa_rl:FA_RL_interface, num_episode: int, len_episode: int, \n",
    "                  alpha: float, gamma: float, get_action: Callable[[int], int],linear_FA: linearFA):\n",
    "    # initiate value function\n",
    "    i = 0\n",
    "    while i < num_episode:\n",
    "        j = 0\n",
    "        s_cur = fa_rl.init_state_gen()\n",
    "        while j < len_episode:\n",
    "            act = get_action(s_cur)\n",
    "            s_next, reward = fa_rl.gen_state_reward(s_cur,act)\n",
    "            target = reward + fa_rl.gamma * linear_FA.v_func_predict(fa_rl.gen_features(s_cur))\n",
    "            linear_FA.update_params(fa_rl.gen_features(s_cur), target)\n",
    "            s_cur = s_next\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return linear_FA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD($\\lambda$) Prediction algorithm with Value Function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_backward(fa_rl:FA_RL_interface, num_episode: int, len_episode: int,\n",
    "               alpha: float, gamma: float, _lambda: float, \n",
    "               get_action: Callable[[int], int],linear_FA: linearFA) -> dict:\n",
    "            \n",
    "    i = 0\n",
    "    while i < num_episode:\n",
    "        j = 0\n",
    "        et = [np.zeros_like(p) for p in linear_FA.get_params()]\n",
    "        s_cur = fa_rl.init_state_gen()\n",
    "        while j < len_episode:\n",
    "            act = get_action(s_cur)\n",
    "            s_next, reward = fa_rl.gen_state_reward(s_cur,act)\n",
    "            target = reward + fa_rl.gamma * linear_FA.v_func_predict(fa_rl.gen_features(s_next))\n",
    "            delta = target - linear_FA.v_func_predict(fa_rl.gen_features(s_cur))\n",
    "            et = [et[i] * gamma * _lambda + fa_rl.gen_features(s_cur) for i in range(len(et))]\n",
    "            for e in et:\n",
    "                linear_FA.update_params(-e * delta)\n",
    "            s_cur = s_next\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return linear_FA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA with Value Function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_features(S:int,A:int):\n",
    "    return np.asarray[S,A] # define features\n",
    "\n",
    "def epsilon_greedy(fa_rl:FA_RL_interface, S:int, eps:float,linear_FA) -> np.ndarray:\n",
    "    pol = eps*np.ones(fa_rl.get_action(S))/len(fa_rl.get_action(S))\n",
    "    A_ast = np.argmax([linear_FA.v_func_prediction(S,a) for a in fa_rl.get_action(S)])\n",
    "    pol[A_ast] += 1-eps\n",
    "    return pol\n",
    "\n",
    "def sarsa(fa_rl:FA_RL_interface, num_episode: int, len_episode: int, \n",
    "          alpha: float, gamma: float, epsilon_greedy: Callable[[int], int], eps: float,linear_FA: linearFA):\n",
    "    \n",
    "    i = 0\n",
    "    while i < num_episode:\n",
    "        j = 0\n",
    "        s_cur = fa_rl.init_state_gen()\n",
    "        pol = epsilon_greedy(fa_rl, s_cur, eps,linear_FA)\n",
    "        u = np.random.uniform(0,1)\n",
    "        cdf = np.cumsum(pol)\n",
    "        act_cur = np.where(cdf > u)[0][0]\n",
    "        \n",
    "        while j < len_episode:\n",
    "            s_next, reward = ra_rl.next_state_gen(s_cur,act)\n",
    "            pol = epsilon_greedy(fa_rl, s_next, eps, linear_FA)\n",
    "            u = np.random.uniform(0,1)\n",
    "            cdf = np.cumsum(pol)\n",
    "            act_next = np.where(cdf > u)[0][0]\n",
    "            f_cur = q_features(s_cur,act_cur)\n",
    "            f_next = q_features(s_next,act_next)\n",
    "            linear_FA.update_params(f_next, linear_FA.v_func_predict(f_cur))\n",
    "            s_cur = s_next\n",
    "            act_cur = act_next\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return linear_FA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA($\\lambda$) with Value Function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_features(S:int,A:int):\n",
    "    return np.asarray[S,A] # define features\n",
    "\n",
    "def epsilon_greedy(fa_rl:FA_RL_interface, S:int, eps:float) -> np.ndarray:\n",
    "    pol = eps*np.ones(fa_rl.get_action(S))/len(fa_rl.get_action(S))\n",
    "    A_ast = np.argmax([linear_FA.v_func_prediction(S,a) for a in fa_rl.get_action(S)])\n",
    "    pol[A_ast] += 1-eps\n",
    "    return pol\n",
    "\n",
    "def sarsa_backward(fa_rl:FA_RL_interface, num_episode: int, len_episode: int, \n",
    "                   alpha: float, gamma: float, _lambda: float, epsilon_greedy: Callable[[int], int], \n",
    "                   eps: float,linear_FA: linearFA):\n",
    "    i = 0\n",
    "    while i < num_episode:\n",
    "        j = 0\n",
    "        s_cur = tb_rl.init_state_gen()\n",
    "        s_cur = fa_rl.init_state_gen()\n",
    "        pol = epsilon_greedy(fa_rl, s_cur, eps, linear_FA)\n",
    "        u = np.random.uniform(0,1)\n",
    "        cdf = np.cumsum(pol)\n",
    "        a_cur = np.where(cdf > u)[0][0]\n",
    "        e_t = [np.zeros_like(p) for p in linear_FA.get_params()]\n",
    "        while j < len_episode:\n",
    "            s_next, reward = fa_rl.next_state_gen(s_cur,a_cur)\n",
    "            pol = epsilon_greedy(fa_rl, s_next, eps, linear_FA)\n",
    "            u = np.random.uniform(0,1)\n",
    "            cdf = np.cumsum(pol)\n",
    "            a_next = np.where(cdf > u)[0][0]\n",
    "            target = reward + fa_rl.gamma * linear_FA.v_func_predict(q_features(s_next,a_next))\n",
    "            delta = target - linear_FA.v_func_predict(q.gen_features(s_cur,a_cur))\n",
    "            et = [et[i] * gamma * _lambda + q_features(s_cur,a_cur) for i in range(len(et))]\n",
    "            for e in et:\n",
    "                linear_FA.update_params(-e * delta)\n",
    "            s_cur = s_next\n",
    "            a_cur = a_next\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return linear_FA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning with Value Function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(fa_rl:FA_RL_interface, S:int, linear_FA) -> int:\n",
    "    pol = eps*np.ones(fa_rl.get_action(S))/len(fa_rl.get_action(S))\n",
    "    A_ast = np.argmax([linear_FA.v_func_prediction(S,a) for a in fa_rl.get_action(S)])\n",
    "    return A_ast\n",
    "\n",
    "def q_learning(fa_rl:FA_RL_interface, num_episode: int, len_episode: int, \n",
    "               alpha: float, gamma: float, epsilon_greedy: Callable[[int], int], greedy: Callable[[int], int], \n",
    "               eps: float, linear_FA) -> dict:\n",
    "    \n",
    "    # Initiate q_func table\n",
    "    i = 0\n",
    "    while i < num_episode:\n",
    "        j = 0\n",
    "        s_cur = tb_rl.init_state_gen()\n",
    "        while j < len_episode:\n",
    "            pol = epsilon_greedy(fa_rl, s_cur, eps,linear_FA)\n",
    "            u = np.random.uniform(0,1)\n",
    "            cdf = np.cumsum(pol)\n",
    "            act_cur = np.where(cdf > u)[0][0]\n",
    "            s_next, reward = fa_rl.next_state_gen(s_cur,act_cur)\n",
    "            pol = epsilon_greedy(fa_rl, s_next, eps, linear_FA)\n",
    "            u = np.random.uniform(0,1)\n",
    "            cdf = np.cumsum(pol)\n",
    "            act_next = np.where(cdf > u)[0][0]\n",
    "            f_cur = q_features(s_cur,act_cur)\n",
    "            f_next = q_features(s_next,act_next)\n",
    "            linear_FA.update_params(f_next, linear_FA.v_func_predict(f_cur))\n",
    "            s_cur = s_next\n",
    "            act_cur = act_next\n",
    "            j += 1\n",
    "        i += 1\n",
    "    \n",
    "    return linear_FA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test in mini project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
